{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a7b334-a4cc-4ec2-a38e-e7a93b4233ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simdjson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import necessary items\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# mostly copied over from MM_Covid_Explain2.ipynb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msimdjson\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msj\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simdjson'"
     ]
    }
   ],
   "source": [
    "# import necessary items\n",
    "# mostly copied over from MM_Covid_Explain2.ipynb\n",
    "import re\n",
    "import simdjson as sj\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import swifter\n",
    "import seaborn as sns\n",
    "import numpy as np\t\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from wutils.general import save_pickle, load_pickle\n",
    "from wutils.mat import MarkedMatrix\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from treeinterpreter import treeinterpreter as ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b9cf97-24dc-402e-8fe7-f6fabf3e42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import new library to use kernels\n",
    "from sklearn.metrics.pairwise import pairwise_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49ae2ec-8c82-4f23-ad40-fb6427932c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to time certain functions that take a long time\n",
    "# not really needed, but somewhat interesting\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72eb7171-370d-4b87-a393-380df9242439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cord-19 (reference) data from pickle\n",
    "# changed from ./pickles/who_cord_df.pkl to ./pickles/newest_cord_df_ft.pkl\n",
    "cord_df = load_pickle('./pickles/newest_cord_df_ft.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8946676b-1ea3-4ec2-80d0-ca9b667a8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load good (true) data from pickle\n",
    "# changed from ./pickles/newest_good_sample_df_ft.pkl to clean_good_df.pkl\n",
    "good_df = load_pickle('./pickles/clean_good_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed9b166b-3f8f-40a9-aa79-01145e112877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bad (false) data from pickle\n",
    "# changed from ./pickles/newest_bad_df_ft.pkl to cleab_bad_df.pkl\n",
    "bad_df = load_pickle('./pickles/clean_bad_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d67d0ef-4618-4e0f-ad14-afc7d661a07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8a9de5ae4c46bda730cc58dd4a3188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/97539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>publishDate</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>covidFreq</th>\n",
       "      <th>sentences</th>\n",
       "      <th>fixed_sent</th>\n",
       "      <th>sent_embeddings</th>\n",
       "      <th>ft_embeddings</th>\n",
       "      <th>mean_embed</th>\n",
       "      <th>hostname</th>\n",
       "      <th>ft_has_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1206633</th>\n",
       "      <td>[Robin Mckie]</td>\n",
       "      <td>2020-04-24 00:00:00</td>\n",
       "      <td>But their impact has been mild compared with t...</td>\n",
       "      <td>Coronavirus: what have scientists learned abou...</td>\n",
       "      <td>https://www.theguardian.com/world/2020/apr/24/...</td>\n",
       "      <td>13</td>\n",
       "      <td>[But their impact has been mild compared with ...</td>\n",
       "      <td>[But their impact has been mild compared with ...</td>\n",
       "      <td>[[-0.15770671, -1.8202316, 0.33610916, -0.9967...</td>\n",
       "      <td>[[-0.020828864, 0.004317664, 0.000431179, 0.04...</td>\n",
       "      <td>[-0.007586088, -0.021697361, 0.0035677361, 0.0...</td>\n",
       "      <td>www.theguardian.com</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               authors          publishDate  \\\n",
       "1206633  [Robin Mckie]  2020-04-24 00:00:00   \n",
       "\n",
       "                                                 cleanText  \\\n",
       "1206633  But their impact has been mild compared with t...   \n",
       "\n",
       "                                                     title  \\\n",
       "1206633  Coronavirus: what have scientists learned abou...   \n",
       "\n",
       "                                                       url  covidFreq  \\\n",
       "1206633  https://www.theguardian.com/world/2020/apr/24/...         13   \n",
       "\n",
       "                                                 sentences  \\\n",
       "1206633  [But their impact has been mild compared with ...   \n",
       "\n",
       "                                                fixed_sent  \\\n",
       "1206633  [But their impact has been mild compared with ...   \n",
       "\n",
       "                                           sent_embeddings  \\\n",
       "1206633  [[-0.15770671, -1.8202316, 0.33610916, -0.9967...   \n",
       "\n",
       "                                             ft_embeddings  \\\n",
       "1206633  [[-0.020828864, 0.004317664, 0.000431179, 0.04...   \n",
       "\n",
       "                                                mean_embed  \\\n",
       "1206633  [-0.007586088, -0.021697361, 0.0035677361, 0.0...   \n",
       "\n",
       "                    hostname  ft_has_nan  \n",
       "1206633  www.theguardian.com       False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of good article\n",
    "good_df[good_df.cleanText.swifter.apply(lambda x: 'Virus-ridden particles are inhaled by others and come into contact with cells' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31be1b0a-6d6c-497b-bea1-16fede7fb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse urls\n",
    "import urllib.parse\n",
    "\n",
    "def get_hostname(url):\n",
    "    o = urllib.parse.urlsplit(url)\n",
    "    return o.hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e2bd051-666a-478f-a664-fd131f60f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns true if text has to do with keep_if (wuhan in this case)\n",
    "# returns false if text contains words in blacklist\n",
    "# if text contains words from neither, defaults to true\n",
    "def news_filter(text):\n",
    "    keep_if = ['wuhan']\n",
    "    text = text.lower()\n",
    "    blacklist = ['u.s.', 'america', 'korea', 'china', 'mexico', 'australia', 'uk', 'u.k.', 'new york', 'los angeles', 'nfl', 'nba', 'mlb', 'epl']\n",
    "    if any(x in text for x in keep_if):\n",
    "        return True\n",
    "    if any(x in text for x in blacklist):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad1309e-1d1a-42a6-8467-a7c9cdae3461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16e733862424c519a6b1b3cbc743142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/97539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb5de433d1a48fe803e60ddc0f2edae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/21135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# not really sure what any of the below does to be honest\n",
    "# I assume it has to do with preparing the data somehow\n",
    "good_df['hostname'] = good_df['url'].swifter.apply(get_hostname)\n",
    "bad_df['hostname'] = bad_df['url'].swifter.apply(get_hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed35a16-84a1-4e69-9e4d-d656c7b933ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c00b6ed7dce4d9aa758c81bc299297c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/97539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808f38297f614898b1885187ebf5ea60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/21135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "good_df['mean_embed'] = good_df['sent_embeddings'].swifter.apply(lambda x: x.mean(axis=0))\n",
    "bad_df['mean_embed'] = bad_df['sent_embeddings'].swifter.apply(lambda x: x.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a314a7cb-a017-4ade-a5c9-8546d3bef9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce5f9c0609646c5879239a5e47ea595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/21135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:  6268\n"
     ]
    }
   ],
   "source": [
    "# seems that we are keeping the bad articles only if they have this \"is ap\" quality\n",
    "# I'm not sure what \"is ap\" means, but that's what this function does\n",
    "is_AP = bad_df['cleanText'].swifter.apply(lambda x: '(AP)' in x)\n",
    "print('Found: ', is_AP.sum())\n",
    "bad_df = bad_df[~is_AP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99aca4fd-b513-48e6-ac48-eb48b9aa1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword\n",
    "KW = 'transmission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4ede1c8-f224-4014-b39e-b3448312811c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e1b4f059f546c3b367d1767b8a5c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/10149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# don't know what this does\n",
    "# more data prep I guess\n",
    "cord_df = cord_df.dropna()\n",
    "cord_df['mean_embed'] = cord_df['sent_embeddings'].swifter.apply(lambda x: x.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caac23da-651f-47ef-904e-9ddca22e2286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab7e3ccb1d4434da6e5a4b3b2ff60a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/10149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf659f231c2944038a3edc71a9af8920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/10149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filtering dataframe to look for the keyword\n",
    "# don't know why will named this \"has_vax\" if the keyword is transmission\n",
    "# but I won't question it\n",
    "has_vax = cord_df[cord_df.title.swifter.apply(lambda x: KW in x.lower()) | cord_df.abstract.swifter.apply(lambda x: KW in x.lower())]\n",
    "vax_ref = np.vstack(has_vax['mean_embed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea50c87-f84e-4712-b687-66096343b990",
   "metadata": {},
   "source": [
    "## From the paper:\n",
    "\n",
    "\"We extract only articles that contain the word “transmission” and name it the “transmission news” dataset. The purpose ... is to provide a smaller sample with articles focusing more on attributes of the virus rather than on other topics.\"\n",
    "\n",
    "The results for these smaller more specific datasets differs from the overall dataset as well, with the transmission news dataset seeming to fare a little bit worse than the other ones.\n",
    "\n",
    "To show the effectiveness of his algorithm, Will is choosing to use the *\"transmission news\"* dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a7e57-f8ff-4dc4-a0ba-0292bd3974ea",
   "metadata": {},
   "source": [
    "![table1](../../table_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30948158-e77d-4ddb-af67-1ba15b76e761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94de62b17384287bfd692d7f3083bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/97539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e6525864724c7a9ff147040fc702b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/14867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# will commented this out\n",
    "# vax_good_df = good_df[good_df['cleanText'].swifter.apply(lambda x: KW in x.lower())]\n",
    "# vax_bad_df = bad_df[bad_df['cleanText'].swifter.apply(lambda x: KW in x.lower())]\n",
    "\n",
    "# creating dataframes and cleaning them up\n",
    "vax_good_df = good_df\n",
    "vax_bad_df = bad_df\n",
    "\n",
    "# applying news filter function:\n",
    "# (removing text containing unwanted keywords, unless they also contain other wanted keywords)\n",
    "vax_good_df = vax_good_df[vax_good_df['cleanText'].swifter.apply(news_filter)]\n",
    "vax_bad_df = vax_bad_df[vax_bad_df['cleanText'].swifter.apply(news_filter)]\n",
    "\n",
    "# turning this into a matrix I think\n",
    "vax_good_mat = np.vstack(vax_good_df.mean_embed)\n",
    "vax_bad_mat = np.vstack(vax_bad_df.mean_embed)\n",
    "\n",
    "# turning matrix into a marked matrix (matrix with labels)\n",
    "# will's own personal python library actually, impressive\n",
    "vax_raw_mm = MarkedMatrix([('good', vax_good_mat), ('bad', vax_bad_mat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e8b26eb-dee3-4438-9de2-3f1c5189673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done w/ good\n",
      "done w/ bad\n"
     ]
    }
   ],
   "source": [
    "# will calculates the distances between reference dataframe and to-be-labeled dataframe here I am assuming\n",
    "# below is a link to possible other metrics to use besides cosine\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html\n",
    "\n",
    "# vgood2ref is the distance between vax_good_mat and vax_ref\n",
    "vgood2ref = cdist(vax_good_mat, vax_ref, 'cosine')\n",
    "print('done w/ good')\n",
    "\n",
    "# vbad2ref is the distance between vax_bad_mat and vax_ref\n",
    "vbad2ref = cdist(vax_bad_mat, vax_ref, 'cosine')\n",
    "print('done w/ bad')\n",
    "\n",
    "# turning this into a marked matrix\n",
    "vax_mm = MarkedMatrix([('good', vgood2ref), ('bad', vbad2ref)])\n",
    "\n",
    "# theoretically, vgood2ref should contain small distances (since good articles should be similar to CORD-19)\n",
    "# and vbad2ref should contain large distances (since bad articles should not be similar to CORD-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82aa1c6b-7558-4b21-9e91-c6afcaff565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt method\n",
    "# commented out by Will originally\n",
    "# has_vax = cord_df.sample(5000)\n",
    "# vax_ref = np.vstack(has_vax['mean_embed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3f1e03c-13c6-488f-ae27-0e720a0145ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# War on Terror Cells: Strategies to Eradicate \"Novel Coronavirus\" Effectively\n",
    "np.where(~np.isfinite(vax_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef302ca8-12e8-4808-b2d0-43fdaee31c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing random forest classifiers to be used later\n",
    "# random forest: a collection of decision trees, where the final decision is the average of all the trees\n",
    "# the idea is: the crowd is smarter\n",
    "raw_tmodel = RandomForestClassifier(n_estimators=500, n_jobs=28)\n",
    "tmodel = RandomForestClassifier(n_estimators=500, n_jobs=28)\n",
    "tmodel2 = RandomForestClassifier(n_estimators=1500, n_jobs=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7db430ae-c793-48db-95b4-5d6f5025bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Prices of alcohols used in making hand sanitizers capped under Essential Commodities Act',\n",
       " 'The government on Thursday said prices of alcohols used in manufacturing hand sanitizers have been capped under the Essential Commodities Act on account of coranavirus outbreak.The decision would empower the central government and states/union territories to regulate prices, production, sale, distribution, transport, movement, storage, information of alcohols used in manufacturing hand sanitizers, used as preventive measure to avoid infection from COVID-19 , Ministry of Consumer Affairs said in a statement.It would also empower governments to smoothen the sale and availability of these items and carry out operations against hoarders, speculators, profiteers, black marketeers and those involved in contravention of the orders of central government and states/UTs in respect of these alcohols.It will help manufacturers of hand sanitizers to keep prices of their product at reasonable level and within the reach of the common people, it added.The ministry said that government has notified an order under the Essential Commodities Act to declare price cap on these alcohols prevailing as on March 5 up to June 30, 2019.\"In view the ongoing outbreak of COVID-19 and concern of the logistics for COVID 19 management particularly during last couple of weeks and that prices of the alcohol used in manufacturing the hand sanitizers have been exorbitantly increased by the producers of such alcohol, government has notified an order under the Act to declare price cap prevailing as on 05.03.2020 on the above alcohols up to 30th June, 2019,\" it said.It has also notified that the raw materials used in manufacture of essential commodities are also essential commodities under the Act.States can now ask the manufactures of these alcohols not to increase the prices of their produce without concurrence of the central government, it said.\"On these items, states may now notify the central order in their official Gazette, also issue their own orders under the Act to that effect and take necessary actions as per the situation prevailing in the respective states,\" it said.Under the Act, powers of the central government have already been delegated to the states by way of orders during 1972 to 1978, it said.The central government, it said, has requested the states/UTs to advise manufacturers of deodorants to manufacture alcohol-based hand sanitizer spray with ethyl alcohol as main content on mass scale in the same plant which may manufacture this alcohol-based sanitizer at very low cost and make it available in the market at the earliest.The states/manufacturers of deodorants may contact drugs controller, Daman & Diu to resolve the issues related to technology.The states have also been advised to ensure licences and approvals for the same on top priority basis. As this item would be an additional product in the production line, it may be endorsed on the drugs and cosmetics licence of such deodorant manufacturers.\"The states are also advised that manufacturer of this product may also be given excise permit for use of ethanol alcohol as raw material,\" it added.Further, they have been advised that the existing manufacturing units of alcohol based sanitizer be encouraged to increase their capacity.\"Ministry of Environment has allowed increase in capacity by 50 per cent without further environment clearances,\" it added.On March 13, the government declared masks, including the N95 variant, and hand sanitizers as \\'essential commodities\\' in the wake of the coronavirus scare leading to shortages and black marketing of these items.')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding some examples of good text\n",
    "tidx = 263\n",
    "vax_good_df.iloc[tidx].title, vax_good_df.iloc[tidx].cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d86961ad-f1df-4693-b7ef-ba34f5931402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function written by will to show program's classification decision process for individual documents\n",
    "def tree_explain_good(idx):\n",
    "    pred, bias, contrib = ti.predict(tmodel, vgood2ref[idx, :].reshape((1, -1)))\n",
    "    print('CLEAN TEXT: ', vax_good_df.iloc[idx].cleanText)\n",
    "    cmat = contrib.reshape((-1, 2))\n",
    "    print('Prediction: ', pred)\n",
    "    print('Bias: ', bias)\n",
    "    import_bad = np.argsort(cmat[:, 0])[::-1]\n",
    "    import_good = np.argsort(cmat[:, 1])[::-1]\n",
    "    print('===== Towards Good: =====')\n",
    "    for idx in import_good[:5]:\n",
    "        print('-'*45)\n",
    "        print(f'CONTRIB: {cmat[idx]}')\n",
    "        print(f'ABSTRACT: {has_vax.iloc[idx].abstract}')\n",
    "        print(f'TITLE: {has_vax.iloc[idx].title}')\n",
    "    print('===== Towards Bad: =====')\n",
    "    for idx in import_bad[:5]:\n",
    "        print('-'*45)\n",
    "        print(f'CONTRIB: {cmat[idx]}')\n",
    "        print(f'ABSTRACT: {has_vax.iloc[idx].abstract}')\n",
    "        print(f'TITLE: {has_vax.iloc[idx].title}')\n",
    "\n",
    "# pretty much a copy of the previous function, except used to compare with bad dataset rather than good\n",
    "def tree_explain_bad(idx):\n",
    "    pred, bias, contrib = ti.predict(tmodel, vbad2ref[idx, :].reshape((1, -1)))\n",
    "    print('CLEAN TEXT: ', vax_bad_df.iloc[idx].cleanText)\n",
    "    cmat = contrib.reshape((-1, 2))\n",
    "    print('Prediction: ', pred)\n",
    "    print('Bias: ', bias)\n",
    "    import_bad = np.argsort(cmat[:, 0])[::-1]\n",
    "    import_good = np.argsort(cmat[:, 1])[::-1]\n",
    "    print('===== Towards Bad: =====')\n",
    "    for idx in import_bad[:5]:\n",
    "        print('-'*45)\n",
    "        print(f'CONTRIB: {cmat[idx]}')\n",
    "        print(f'ABSTRACT: {has_vax.iloc[idx].abstract}')\n",
    "        print(f'TITLE: {has_vax.iloc[idx].title}')\n",
    "    print('===== Towards Good: =====')\n",
    "    for idx in import_good[:5]:\n",
    "        print('-'*45)\n",
    "        print(f'CONTRIB: {cmat[idx]}')\n",
    "        print(f'ABSTRACT: {has_vax.iloc[idx].abstract}')\n",
    "        print(f'TITLE: {has_vax.iloc[idx].title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1439e79f-dd9c-4f41-baab-b7b8a90e8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was commented out by will\n",
    "# vax_mm.mat[~np.isfinite(vax_mm.mat)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1aa44-34a6-446d-9d06-e4076dd9d1f1",
   "metadata": {},
   "source": [
    "**Training and evaluation begins after this point.**\n",
    "\n",
    "*Quick Refresher:*\n",
    "* vax_mm was defined above as a marked matrix of vgood2ref as good, and vbad2ref as bad\n",
    "* vgood2ref is the matrix of distances between the good matrix and the vax ref\n",
    "* vbad2ref is the matrix of distances between the bad matrix and the vax ref\n",
    "* ideally, vgood2ref should have small distances\n",
    "* while vbad2ref whould have large distances\n",
    "\n",
    "**Note:** *single_split_classify* is a function from will's wutils python library.\n",
    "\n",
    "The source can be found here: https://github.com/willshiao/wutils/blob/master/wutils/mat.py\n",
    "\n",
    "The numbers that single_split_classify returns are in the following format: ((accuracy, f1-score), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e6e4fd8-0589-44ab-bc1d-d4c048b92a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results from running program on vax_raw_mm\n",
    "rawResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae26e173-f7db-41d3-9804-4663cad5d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== RUN FIRST TIME WITH COSINE ======\n",
    "cosineResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60c74a34-202b-4e97-b756-42153cd1d3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_1:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9103275460891355, 0.9516689176152807), {'bad': 0, 'good': 1})\n",
      "seconds:  76.65508675575256\n"
     ]
    }
   ],
   "source": [
    "# run vax_mm with tmodel\n",
    "print('trial_1:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel, return_labels=True)\n",
    "print(res)\n",
    "cosineResults['trial_1'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb2cc188-13bf-4928-bea1-6376e1296e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_2:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9133703239663504, 0.9535775944753501), {'bad': 0, 'good': 1})\n",
      "seconds:  79.59276962280273\n"
     ]
    }
   ],
   "source": [
    "# running vax_mm with tmodel again\n",
    "print('trial_2:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel, return_labels=True)\n",
    "print(res)\n",
    "cosineResults['trial_2'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2adfcb-57fa-42dc-85fe-96fec30a46b7",
   "metadata": {},
   "source": [
    "**Important Note:** \n",
    "\n",
    "trial_3 is a bit different from the other trials.\n",
    "\n",
    "Unlike the other trials, trial_3 uses *vax_raw_mm* instead of *vax_mm.* vax_raw_mm contains the absolute positions of each article instead of the distances from each article to the collection of reference articles. \n",
    "\n",
    "After running the below code, I was able to obtain an accuracy of roughly 0.92 and f1-score of roughly 0.96. I have no idea how the results can be so good with just the absolute positions of the documents and not the distances, (since the distances are what determine how likely it is that each document is \"good\" or \"bad\"), but I suppose that this is a question for Will.\n",
    "\n",
    "Most importantly though, when we are comparing the effectiveness of different kernels or different distance calculation equations, we should ignore this trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "402bcb5c-12d3-442f-892d-109cec8e540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_3:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9205298013245033, 0.9574549635875814), {'bad': 0, 'good': 1})\n",
      "seconds:  72.37524795532227\n"
     ]
    }
   ],
   "source": [
    "# running vax_raw_mm with raw_t_model\n",
    "print('trial_3:')\n",
    "start = time.time()\n",
    "res = vax_raw_mm.single_split_classify(raw_tmodel, return_labels=True)\n",
    "print(res)\n",
    "rawResults['trial_3'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cbb175f-0991-4f89-8d7d-502fa15a2873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_4:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9126543762305351, 0.9530949634755864), {'bad': 0, 'good': 1})\n",
      "seconds:  232.05667996406555\n"
     ]
    }
   ],
   "source": [
    "# running vax_mm with tmodel2\n",
    "print('trial_4:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel2, return_labels=True)\n",
    "print(res)\n",
    "cosineResults['trial_4'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6546b-262e-42a7-8520-d96d0cae00f3",
   "metadata": {},
   "source": [
    "**trial_5 also uses vax_raw_mm and so should also be ignored when comparing the effectiveness of different kernels/distance calculations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "034dfc54-4157-4017-9d54-fb4d973a464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_5:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9115804546268123, 0.9524862941233048), {'bad': 0, 'good': 1})\n",
      "seconds:  217.21354818344116\n"
     ]
    }
   ],
   "source": [
    "# running vax_raw_mm with tmodel2\n",
    "print('trial_5:')\n",
    "start = time.time()\n",
    "res = vax_raw_mm.single_split_classify(tmodel2, return_labels=True)\n",
    "print(res)\n",
    "rawResults['trial_5'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4374bc6-cdbd-4fb3-962f-df037aa85d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "trial_1\n",
      "accuracy:  0.9103275460891355\n",
      "f1-score:  0.9516689176152807\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_2\n",
      "accuracy:  0.9133703239663504\n",
      "f1-score:  0.9535775944753501\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_4\n",
      "accuracy:  0.9126543762305351\n",
      "f1-score:  0.9530949634755864\n",
      "labels:  {'bad': 0, 'good': 1}\n"
     ]
    }
   ],
   "source": [
    "# print the contents of cosineResults\n",
    "for e in cosineResults:\n",
    "    print('===================')\n",
    "    print(e)\n",
    "    print('accuracy: ', cosineResults[e][0][0])\n",
    "    print('f1-score: ', cosineResults[e][0][1])\n",
    "    print('labels: ', cosineResults[e][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cfc404-afd6-4ce8-abe1-f3a73b835d97",
   "metadata": {},
   "source": [
    "**Changing from a simple distance calculation to a kernel after this point.**\n",
    "\n",
    "From this point on, we are going to be trying out a bunch of kernels to see if they work better than a simple cosine distance calculation. I don't really fully understand all the math behind how these kernels work, but now we're about to see if they are better.\n",
    "\n",
    "The documentation for pairwise_kernels states that \"If the input is a kernel matrix, it is returned instead.\" While we are using vax_good_mat as a matrix, it is technically a Python array data structure, and so it won't get automatically returned. pairwise_kernel will indeed perform an operation on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a5a05c8-52db-4735-a818-d9f989b32064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done w/ good\n",
      "done w/ bad\n"
     ]
    }
   ],
   "source": [
    "# documentation for pairwise_kernels found here:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html\n",
    "# I don't think that the order of which matrix is first matters, but we can test with different arrangements\n",
    "# possible kernels: [‘additive_chi2’, ‘chi2’, ‘linear’, ‘poly’, ‘polynomial’, ‘rbf’, ‘laplacian’, ‘sigmoid’, ‘cosine’]\n",
    "\n",
    "vgood2ref = pairwise_kernels(vax_good_mat, vax_ref, metric='polynomial')\n",
    "print('done w/ good')\n",
    "\n",
    "vbad2ref = pairwise_kernels(vax_bad_mat, vax_ref, metric='polynomial')\n",
    "print('done w/ bad')\n",
    "\n",
    "vax_mm = MarkedMatrix([('good', vgood2ref), ('bad', vbad2ref)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "429ef532-c4a0-4624-84cd-819089d3a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== RUN WITH POLYNOMIAL KERNEL ======\n",
    "polyKernelResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98a38d9e-83de-4ad1-a862-abaf97277710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_1:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9097905852872741, 0.9514076359429232), {'bad': 0, 'good': 1})\n",
      "seconds:  58.91968321800232\n"
     ]
    }
   ],
   "source": [
    "print('trial_1:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel, return_labels=True)\n",
    "print(res)\n",
    "polyKernelResults['trial_1'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23351b8d-6f6c-427f-a412-752a693fddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_2:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9081797028816896, 0.9505446833124459), {'bad': 0, 'good': 1})\n",
      "seconds:  61.162139892578125\n"
     ]
    }
   ],
   "source": [
    "# running vax_mm with tmodel again\n",
    "print('trial_2:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel, return_labels=True)\n",
    "print(res)\n",
    "polyKernelResults['trial_2'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c3ad9a0-c96e-4c3c-8daa-fd40c8830991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_4:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9088956506175049, 0.9510906120880177), {'bad': 0, 'good': 1})\n",
      "seconds:  178.65489196777344\n"
     ]
    }
   ],
   "source": [
    "# skip trial_3 and trial_5 since those use vax_raw_mm\n",
    "# running vax_mm with tmodel2\n",
    "print('trial_4:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel2, return_labels=True)\n",
    "print(res)\n",
    "polyKernelResults['trial_4'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0aabd6-e4b4-48c2-b7c5-4d495d3d4937",
   "metadata": {},
   "source": [
    "**Side Note:**\n",
    "\n",
    "Not the information I was looking for originally, but it seems that using a kernel reduces the training + evaluation time by a noticable amount, with the time reduction being proportional to the total time taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f0fc9e5-a694-42e9-9c5e-6c29935c38a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "trial_1\n",
      "accuracy:  0.9097905852872741\n",
      "f1-score:  0.9514076359429232\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_2\n",
      "accuracy:  0.9081797028816896\n",
      "f1-score:  0.9505446833124459\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_4\n",
      "accuracy:  0.9104170395561124\n",
      "f1-score:  0.9517660097335325\n",
      "labels:  {'bad': 0, 'good': 1}\n"
     ]
    }
   ],
   "source": [
    "# print the contents of polyKernelResults\n",
    "for e in polyKernelResults:\n",
    "    print('===================')\n",
    "    print(e)\n",
    "    print('accuracy: ', polyKernelResults[e][0][0])\n",
    "    print('f1-score: ', polyKernelResults[e][0][1])\n",
    "    print('labels: ', polyKernelResults[e][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67dc7438-8156-4512-829e-d40e4109ed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: positive difference means that poly-kernel is better.\n",
      "===================\n",
      "trial_1\n",
      "poly-kernel accuracy:  0.9097905852872741\n",
      "cosine accuracy:  0.9103275460891355\n",
      "difference:  -0.0005369608018613858\n",
      "poly-kernel f1-score:  0.9514076359429232\n",
      "cosine f1-score:  0.9516689176152807\n",
      "difference:  -0.0002612816723575051\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_2\n",
      "poly-kernel accuracy:  0.9081797028816896\n",
      "cosine accuracy:  0.9133703239663504\n",
      "difference:  -0.00519062108466084\n",
      "poly-kernel f1-score:  0.9505446833124459\n",
      "cosine f1-score:  0.9535775944753501\n",
      "difference:  -0.0030329111629041616\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_4\n",
      "poly-kernel accuracy:  0.9104170395561124\n",
      "cosine accuracy:  0.9126543762305351\n",
      "difference:  -0.0022373366744227186\n",
      "poly-kernel f1-score:  0.9517660097335325\n",
      "cosine f1-score:  0.9530949634755864\n",
      "difference:  -0.0013289537420538844\n",
      "labels:  {'bad': 0, 'good': 1}\n"
     ]
    }
   ],
   "source": [
    "# compare results of polyKernel with results of cosine:\n",
    "print('Note: positive difference means that poly-kernel is better.')\n",
    "for e in polyKernelResults:\n",
    "    print('===================')\n",
    "    print(e)\n",
    "    print('poly-kernel accuracy: ', polyKernelResults[e][0][0])\n",
    "    print('cosine accuracy: ', cosineResults[e][0][0])\n",
    "    print('difference: ', polyKernelResults[e][0][0] - cosineResults[e][0][0])\n",
    "    print('poly-kernel f1-score: ', polyKernelResults[e][0][1])\n",
    "    print('cosine f1-score: ', cosineResults[e][0][1])\n",
    "    print('difference: ', polyKernelResults[e][0][1] - cosineResults[e][0][1])\n",
    "    print('labels: ', polyKernelResults[e][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740619d-0ac3-45f2-a57d-2a803229522e",
   "metadata": {},
   "source": [
    "**Polynomial kernel vs cosine function analysis:**\n",
    "\n",
    "It seems that there isn't really a noticable improvement from using a polynomial kernel vs using a simple cosine equation. In fact, in all 3 cases, the cosine function actually happened to be slightly better, although the difference is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5bcb8a2c-e093-42ef-946d-1fbb3280b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== RUN WITH RBF KERNEL ======\n",
    "rbfKernelResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bcca3344-3983-4e84-afa3-3fb019a69708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done w/ good\n",
      "done w/ bad\n"
     ]
    }
   ],
   "source": [
    "vgood2ref = pairwise_kernels(vax_good_mat, vax_ref, metric='rbf')\n",
    "print('done w/ good')\n",
    "\n",
    "vbad2ref = pairwise_kernels(vax_bad_mat, vax_ref, metric='rbf')\n",
    "print('done w/ bad')\n",
    "\n",
    "vax_mm = MarkedMatrix([('good', vgood2ref), ('bad', vbad2ref)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e754dd8d-092f-44a0-9d95-046b68c9b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_1:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9072847682119205, 0.9499613601236476), {'bad': 0, 'good': 1})\n",
      "seconds:  75.75212001800537\n"
     ]
    }
   ],
   "source": [
    "print('trial_1:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel, return_labels=True)\n",
    "print(res)\n",
    "rbfKernelResults['trial_1'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a905fa1-9558-4b1e-bd14-53a6b36e15fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_2:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9119384284947198, 0.9527241279907754), {'bad': 0, 'good': 1})\n",
      "seconds:  75.78960728645325\n"
     ]
    }
   ],
   "source": [
    "# running vax_mm with tmodel again\n",
    "print('trial_2:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel, return_labels=True)\n",
    "print(res)\n",
    "rbfKernelResults['trial_2'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4cb7a09-acf6-4b4f-8acd-cf1e8ae7f4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_4:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9110434938249508, 0.9521701472428062), {'bad': 0, 'good': 1})\n",
      "seconds:  228.27178812026978\n"
     ]
    }
   ],
   "source": [
    "# skip trial_3 and trial_5 since those use vax_raw_mm\n",
    "# running vax_mm with tmodel2\n",
    "print('trial_4:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel2, return_labels=True)\n",
    "print(res)\n",
    "rbfKernelResults['trial_4'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5721a919-40f9-4a15-a2c7-06de4e6a5a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "trial_1\n",
      "accuracy:  0.9072847682119205\n",
      "f1-score:  0.9499613601236476\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_2\n",
      "accuracy:  0.9119384284947198\n",
      "f1-score:  0.9527241279907754\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_4\n",
      "accuracy:  0.9110434938249508\n",
      "f1-score:  0.9521701472428062\n",
      "labels:  {'bad': 0, 'good': 1}\n"
     ]
    }
   ],
   "source": [
    "# print the contents of rbfKernelResults\n",
    "for e in rbfKernelResults:\n",
    "    print('===================')\n",
    "    print(e)\n",
    "    print('accuracy: ', rbfKernelResults[e][0][0])\n",
    "    print('f1-score: ', rbfKernelResults[e][0][1])\n",
    "    print('labels: ', rbfKernelResults[e][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb879758-b92e-4b3e-b8f9-d5ee087a8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: positive difference means that rbf-kernel is better.\n",
      "===================\n",
      "trial_1\n",
      "rbf-kernel accuracy:  0.9072847682119205\n",
      "cosine accuracy:  0.9103275460891355\n",
      "difference:  -0.003042777877214964\n",
      "rbf-kernel f1-score:  0.9499613601236476\n",
      "cosine f1-score:  0.9516689176152807\n",
      "difference:  -0.0017075574916330938\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_2\n",
      "rbf-kernel accuracy:  0.9119384284947198\n",
      "cosine accuracy:  0.9133703239663504\n",
      "difference:  -0.0014318954716305843\n",
      "rbf-kernel f1-score:  0.9527241279907754\n",
      "cosine f1-score:  0.9535775944753501\n",
      "difference:  -0.0008534664845746276\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_4\n",
      "rbf-kernel accuracy:  0.9110434938249508\n",
      "cosine accuracy:  0.9126543762305351\n",
      "difference:  -0.0016108824055843796\n",
      "rbf-kernel f1-score:  0.9521701472428062\n",
      "cosine f1-score:  0.9530949634755864\n",
      "difference:  -0.000924816232780179\n",
      "labels:  {'bad': 0, 'good': 1}\n"
     ]
    }
   ],
   "source": [
    "# compare results of rbfKernel with results of cosine:\n",
    "print('Note: positive difference means that rbf-kernel is better.')\n",
    "for e in rbfKernelResults:\n",
    "    print('===================')\n",
    "    print(e)\n",
    "    print('rbf-kernel accuracy: ', rbfKernelResults[e][0][0])\n",
    "    print('cosine accuracy: ', cosineResults[e][0][0])\n",
    "    print('difference: ', rbfKernelResults[e][0][0] - cosineResults[e][0][0])\n",
    "    print('rbf-kernel f1-score: ', rbfKernelResults[e][0][1])\n",
    "    print('cosine f1-score: ', cosineResults[e][0][1])\n",
    "    print('difference: ', rbfKernelResults[e][0][1] - cosineResults[e][0][1])\n",
    "    print('labels: ', rbfKernelResults[e][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77451694-8d12-44c4-b20f-7d63598de36a",
   "metadata": {},
   "source": [
    "**RBF kernel vs cosine function analysis:**\n",
    "\n",
    "It seems that once again, there isn't really a noticable improvement from using an rbf kernel vs using a simple cosine equation. Furthermore, once again in all 3 cases, the cosine function actually happened to be slightly better, although the difference is again negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec89f14f-b7c2-4633-af89-3c6b2f84e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== RUN WITH LAPLACIAN KERNEL ======\n",
    "laplacianKernelResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "643b5908-6cc1-42b3-9511-9729f1e393dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done w/ good\n",
      "done w/ bad\n"
     ]
    }
   ],
   "source": [
    "vgood2ref = pairwise_kernels(vax_good_mat, vax_ref, metric='laplacian')\n",
    "print('done w/ good')\n",
    "\n",
    "vbad2ref = pairwise_kernels(vax_bad_mat, vax_ref, metric='laplacian')\n",
    "print('done w/ bad')\n",
    "\n",
    "vax_mm = MarkedMatrix([('good', vgood2ref), ('bad', vbad2ref)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73b81e28-6383-4c37-ac88-231fffb0b8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_1:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.912833363164489, 0.9532180595581172), {'bad': 0, 'good': 1})\n",
      "seconds:  80.60169792175293\n"
     ]
    }
   ],
   "source": [
    "print('trial_1:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel, return_labels=True)\n",
    "print(res)\n",
    "laplacianKernelResults['trial_1'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05b02fec-968e-46c1-af56-594e964a3917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_2:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9153391802398425, 0.954510482785151), {'bad': 0, 'good': 1})\n",
      "seconds:  77.38251423835754\n"
     ]
    }
   ],
   "source": [
    "# running vax_mm with tmodel again\n",
    "print('trial_2:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel, return_labels=True)\n",
    "print(res)\n",
    "laplacianKernelResults['trial_2'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "942d5be2-77a4-4998-9909-6efb6ece81b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_4:\n",
      "Training model...\n",
      "Evaluating model...\n",
      "((0.9104170395561124, 0.9518680578929655), {'bad': 0, 'good': 1})\n",
      "seconds:  231.2023479938507\n"
     ]
    }
   ],
   "source": [
    "# skip trial_3 and trial_5 since those use vax_raw_mm\n",
    "# running vax_mm with tmodel2\n",
    "print('trial_4:')\n",
    "start = time.time()\n",
    "res = vax_mm.single_split_classify(tmodel2, return_labels=True)\n",
    "print(res)\n",
    "laplacianKernelResults['trial_4'] = res\n",
    "end = time.time()\n",
    "print('seconds: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dca9c044-7e88-4b07-a4b6-22a3e8883ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "trial_1\n",
      "accuracy:  0.912833363164489\n",
      "f1-score:  0.9532180595581172\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_2\n",
      "accuracy:  0.9153391802398425\n",
      "f1-score:  0.954510482785151\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_4\n",
      "accuracy:  0.9104170395561124\n",
      "f1-score:  0.9518680578929655\n",
      "labels:  {'bad': 0, 'good': 1}\n"
     ]
    }
   ],
   "source": [
    "# print the contents of laplacianKernelResults\n",
    "for e in laplacianKernelResults:\n",
    "    print('===================')\n",
    "    print(e)\n",
    "    print('accuracy: ', laplacianKernelResults[e][0][0])\n",
    "    print('f1-score: ', laplacianKernelResults[e][0][1])\n",
    "    print('labels: ', laplacianKernelResults[e][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1de58d96-b1af-4074-9c3e-d881003f8bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: positive difference means that rbf-kernel is better.\n",
      "===================\n",
      "trial_1\n",
      "laplacian-kernel accuracy:  0.912833363164489\n",
      "cosine accuracy:  0.9103275460891355\n",
      "difference:  0.002505817075353578\n",
      "laplacian-kernel f1-score:  0.9532180595581172\n",
      "cosine f1-score:  0.9516689176152807\n",
      "difference:  0.0015491419428365072\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_2\n",
      "laplacian-kernel accuracy:  0.9153391802398425\n",
      "cosine accuracy:  0.9133703239663504\n",
      "difference:  0.001968856273492081\n",
      "laplacian-kernel f1-score:  0.954510482785151\n",
      "cosine f1-score:  0.9535775944753501\n",
      "difference:  0.0009328883098009833\n",
      "labels:  {'bad': 0, 'good': 1}\n",
      "===================\n",
      "trial_4\n",
      "laplacian-kernel accuracy:  0.9104170395561124\n",
      "cosine accuracy:  0.9126543762305351\n",
      "difference:  -0.0022373366744227186\n",
      "laplacian-kernel f1-score:  0.9518680578929655\n",
      "cosine f1-score:  0.9530949634755864\n",
      "difference:  -0.0012269055826209518\n",
      "labels:  {'bad': 0, 'good': 1}\n"
     ]
    }
   ],
   "source": [
    "# compare results of laplacianKernel with results of cosine:\n",
    "print('Note: positive difference means that rbf-kernel is better.')\n",
    "for e in laplacianKernelResults:\n",
    "    print('===================')\n",
    "    print(e)\n",
    "    print('laplacian-kernel accuracy: ', laplacianKernelResults[e][0][0])\n",
    "    print('cosine accuracy: ', cosineResults[e][0][0])\n",
    "    print('difference: ', laplacianKernelResults[e][0][0] - cosineResults[e][0][0])\n",
    "    print('laplacian-kernel f1-score: ', laplacianKernelResults[e][0][1])\n",
    "    print('cosine f1-score: ', cosineResults[e][0][1])\n",
    "    print('difference: ', laplacianKernelResults[e][0][1] - cosineResults[e][0][1])\n",
    "    print('labels: ', laplacianKernelResults[e][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c27813c-85fd-4015-81c8-7164c4a29865",
   "metadata": {},
   "source": [
    "**laplacian kernel vs cosine function analysis:**\n",
    "\n",
    "It seems that for two out of the three trials, the laplacian kernel actually managed to do better than the cosine function, although only slightly. However, for the third trial, the laplacian kernel fared worse, however the difference for all three trials is once again negligable.\n",
    "\n",
    "One thing that is interesting to note is that trial_1 and trial_2 use the same tmodel/Random Forest Classifier, one which contains 500 estimators. However, trial_4 uses a different Random Forest Classifier, tmodel2, which contains 1500 estimators. It seems that the laplacian kernel fared better when the number of estimators was lower however fared worse when the number of estimators was higher. \n",
    "\n",
    "I tried testing with a larger Random Forest Classifier (3000 estimators) but it took an ungodly amount of time to train, to the point where my Jupyter Lab would keep timing out and crashing, so I decided to just scrap the idea."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
